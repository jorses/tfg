{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Libraries and Tools used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Our graphical elements are going to be done with matplotlib, inline graphs\"\"\"\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"CSV: utility to work with the most common format for datastorage : Comma Separated Values\"\"\"\n",
    "import csv\n",
    "\n",
    "\"\"\"JSON: utility to work with JavaScript Object Notation, the original information format storage we \"\"\"\n",
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "\"\"\"Pandas: python's most used library to work with datasets\"\"\"\n",
    "import pandas as pd \n",
    "\n",
    "\"\"\" Type detection for our dataframe \"\"\"\n",
    "from pandas.api.types import (\n",
    "    is_categorical_dtype,\n",
    "    is_datetime64_dtype,\n",
    "    is_object_dtype,\n",
    "    is_numeric_dtype,\n",
    "    is_string_dtype\n",
    ")\n",
    "\n",
    "\"\"\"Numpy: python's most used library to work with large amounts of numbers\"\"\"\n",
    "import numpy as np\n",
    "\"\"\"OS: utility to work with paths and file openings independently of operating system\"\"\"\n",
    "import os\n",
    "\"\"\"Glob: used to look for file extensions inside given folders\"\"\"\n",
    "import glob \n",
    "\n",
    "\"\"\"Candidates for ML predictive model implementations\"\"\"\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Storage module\n",
    "\n",
    "### Methods \n",
    "\n",
    "#### __init__\n",
    "#### print_info\n",
    "#### from_json\n",
    "#### to_json\n",
    "#### update_stats\n",
    "#### load_from_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreDomain():\n",
    "    \n",
    "    def __init__(self, attribute_list, path=os.getcwd(), domain_name=None):\n",
    "        \"\"\" Handy initialization \"\"\"\n",
    "        self.load(attribute_list, path, domain_name)\n",
    "        \n",
    "    def load(self, attribute_list, path, domain_name):\n",
    "        \"\"\" Given a dataset searches for a domain in the specified folder.\n",
    "            If found, loads it into the class itself. If not found loads an empty template.\n",
    "        \"\"\"\n",
    "        for fnm in glob.glob(os.path.join(path,\"*.json\")):\n",
    "            with open(fnm) as f:\n",
    "                data = json.load(f)\n",
    "                print(\"     \", data[\"attribute_list\"], attribute_list)\n",
    "            if data[\"attribute_list\"] == attribute_list:\n",
    "                self.fnm = fnm\n",
    "                self.domain_name = domain_name or str(attribute_list)\n",
    "                self.modify(data)\n",
    "                break;\n",
    "                        \n",
    "        if not hasattr(self,\"fnm\"):\n",
    "            print(\"No matching knowledge found for your domain, setting an empty one\")\n",
    "            self.attribute_list = attribute_list\n",
    "            self.domain_name = domain_name or str(attribute_list)\n",
    "            self.knowledge = {\n",
    "                \"dataset_stats\" : {},\n",
    "                \"column_stats\": {}\n",
    "            }\n",
    "            self.fnm = glob.glob(os.path.join( path, str(self.domain_name)+\".json\") )\n",
    "                    \n",
    "    def save(self):\n",
    "        \"\"\" Saves itself as an object in .json format\n",
    "        \"\"\"\n",
    "        print(\"\" \"\", self.fnm)\n",
    "        with open(self.fnm,'w') as f:\n",
    "            json.dump(self.__dict__, f)\n",
    "            \n",
    "    def modify(self,data):\n",
    "        \"\"\" Modifies the class from a physical representation (dict) of it\n",
    "        \"\"\"   \n",
    "        self.domain_name = data[\"domain_name\"] if \"domain_name\" in data else self.domain_name\n",
    "        self.attribute_list = data[\"attribute_list\"] if \"attribute_list\" in data else self.attribute_list\n",
    "        self.domain_knowledge = data[\"domain_knowledge\"] if \"domain_knowledge\" in data else self.domain_knowledge\n",
    "        self.fnm = data[\"fnm\"] if \"fnm\" in data else self.fnm\n",
    "\n",
    "    def print_info(self):\n",
    "        \"\"\" Util to print attributes\n",
    "        \"\"\"\n",
    "        print(self.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The JSON\n",
    "&nbsp;\n",
    "\n",
    " This object is used to represent the raw information stored.\n",
    "\n",
    " It contains statistics and properties from both columns and datasets, different for different types, as well as how they were measured (by saving the function to be called on a pandas dataset in the case of a dataset stat, or on the values themselves in the case of a function stat).\n",
    "\n",
    " Saving how they're measured is important to measure new datasets and to be able to compare metrics effectively.\n",
    "\n",
    " Both the column specific stats and the dataset stats are subjective to change, and the knowledgedomain object can be modified and adapted to fit new parameters easily.\n",
    "\n",
    " This structure is generated for a single dataset and then combined with the domain one to take account of the new one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_STRUCTURE = {\n",
    "    \"dataset_stats\": {\n",
    "        \"individuals\": 0,\n",
    "        \"max_rows\": {\n",
    "            \"value\": 1400,\n",
    "            \"metric\" : \"count_rows_null\",\n",
    "        }, # alert if new rows are <<<, as different results are skewed\n",
    "        \"avg_rows\": {\n",
    "            \"value\": 1400,\n",
    "            \"metric\" : \"count_rows_null\"\n",
    "        },\n",
    "        \"full_rows\": {\n",
    "            \"value\": 0.97\n",
    "        }\n",
    "    },\n",
    "    \"column_stats\": {\n",
    "        \"a\": {\n",
    "            \"type\": \"numeric\",\n",
    "            \"stats\": {\n",
    "                \"median\": 12.6,\n",
    "                \"std_dev\": 1.3,\n",
    "                \"max\": 25.5,\n",
    "                \"min\": 2.3,\n",
    "                \"NaNs\": 0.02 # as % of dataset\n",
    "            }     \n",
    "        },\n",
    "        \"b\": {\n",
    "            \"type\": \"categorical\",\n",
    "            \"stats\": {\n",
    "                \"most_frequent\": \"cat\",\n",
    "                \"values\": {\n",
    "                    \"cat\": 0.2,\n",
    "                    \"dog\": 0.6\n",
    "                },\n",
    "                \"nan\": 0.1\n",
    "            }\n",
    "            \n",
    "        }\n",
    "\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The analysis module\n",
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The purpose of these functions is to provide a way to measure the properties of a given dataset or knowledge domain.\n",
    "    We can categorize them as follows:\n",
    "    First the \"measurement\" metrics, used to get the information of a single dataset or domain.\n",
    "    \n",
    "    - Dataset Metrics : they concern the dataset as a whole, like number of rows with missing values.\n",
    "        dm :: (ds) --> num\n",
    "        \n",
    "    - Single Column Metrics : they concern a certain column, and are based on the type of the column.\n",
    "      For numerical columns we will have things like median, averages, deviations, distributions...\n",
    "      For categorical columns we'll work with frequencies and things of the sort.\n",
    "        scm :: (col) --> num\n",
    "        \n",
    "    - Multiple Column Metrics : we will be looking for correlations and things of that sort.\n",
    "       scm :: (col,col) --> num \n",
    "       Time based metrics will be defined from this construct.\n",
    "    \n",
    "    We will also have \"comparison\" metrics, used to compare datasets against their domains.\n",
    "    These metrics will compare the output of two measurement metrics, both will have to\n",
    "    spawn from the same function.\n",
    "        comp_m :: (metric) --> num\n",
    "    \n",
    "    Note that these metrics are not to provide \"meaning\" or any human-readable input, nor to be\n",
    "    inherently comparable between each other outside of a framework of understanding of the domain\n",
    "    (metric importance).\n",
    "    \n",
    "    A mean to convert these machine cold metrics into human understanding will be provided in further \n",
    "    modules. For now, we're not taking humans into account.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Dataset Metrics \"\"\"\n",
    "\n",
    "def count_rows_null(df):\n",
    "    return df.isnull().shape(0)\n",
    "\n",
    "\"\"\" Columns \"\"\"\n",
    "\n",
    "\"\"\" For consistency, every column is accepted as a pandas Series \"\"\"\n",
    "\"\"\" Numeric, accepted as pandas Series \"\"\"\n",
    "\n",
    "def median(col):\n",
    "    return np.median(col.values)\n",
    "\n",
    "def average(col):\n",
    "    return np.average(col.values)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" All types \"\"\"\n",
    "\n",
    "def count_not_null(col):\n",
    "    return np.count_nonzero(~np.isnan(data))\n",
    "\n",
    "def count_null(col):\n",
    "    return col.size - count_not_null(col)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Categorical \"\"\"\n",
    "\n",
    "def count_freqs(col):\n",
    "    return 1.2\n",
    "def count_nans(col):\n",
    "    return 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_metric(a):\n",
    "    return median_raw(a)\n",
    "def median_raw(a):\n",
    "    return np.median(a)\n",
    "def most_frequent(a):\n",
    "    return np.bincount(a).argmax()\n",
    "IS_DTYPE = {\n",
    "    \"number\": is_numeric_dtype,\n",
    "    \"category\": is_categorical_dtype,\n",
    "    \"datetime\": is_datetime64_dtype,\n",
    "}\n",
    "\n",
    "DEFAULT_METRICS = { #(type,comparison_metric)\n",
    "    \"numerical\" : median_raw,\n",
    "    \"categorical\": most_frequent,\n",
    "    \"dataset\" : {\n",
    "        \"count_rows\":{\n",
    "            \"metric\": 2\n",
    "        },\n",
    "        \"rows_with_nulls\":{\n",
    "            \"metric\": 3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "COMPARISON_METRICS = { #(metric(name of a function) , comparison_metric)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAnalyzer():\n",
    "    \n",
    "    def __init__(self, df, path):\n",
    "        self.dataset = df            \n",
    "        self.std = StoreDomain(domain_name=df.name, attribute_list=list(df.columns), path=path)\n",
    "        self.domain_knowledge = self.std.domain_knowledge\n",
    "        self.dataset_knowledge = self.get_stats(df)\n",
    "        \n",
    "    def get_stats(self, df):\n",
    "        \"\"\" Populates the stats dictionary \"\"\"\n",
    "        return {\n",
    "            \"column_stats\": self.get_column_stats(df),\n",
    "            \"dataset_stats\": self.get_dataset_stats(df)\n",
    "        }\n",
    "    \n",
    "    def get_column_stats(self,df):\n",
    "        \"\"\" Column stats \"\"\"\n",
    "\n",
    "        stats = {}\n",
    "        col_types = self.get_column_types(df)\n",
    "        \n",
    "        for c in self.get_column_types(df):\n",
    "            if c in self.domain_knowledge[\"column_stats\"]:\n",
    "                func = globals()[self.domain_knowledge[\"column_stats\"][c][\"metric\"]]\n",
    "            else:\n",
    "                func = DEFAULT_METRICS[col_types[c]]\n",
    "                \n",
    "            stats[c] = {\n",
    "                    \"metric\" : func.__name__,\n",
    "                    \"value\" : func(df[c].values)\n",
    "            }\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    def get_dataset_stats(self,df):\n",
    "        \"\"\" Dataset stats \"\"\"\n",
    "        stats = {}\n",
    "        dataset_metrics = self.domain_knowledge[\"dataset_stats\"]\n",
    "        for m in dataset_metrics:\n",
    "            stats[m] = {\n",
    "                \"metric\": globals()[dataset_metrics[m][\"metric\"]],\n",
    "                \"value\": globals()[dataset_metrics[m][\"metric\"]](df)\n",
    "            }\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    def get_column_types(self, df):\n",
    "        \"\"\" Get column types \"\"\"\n",
    "        col_types = {}\n",
    "        for c in df.columns:\n",
    "            col_types[c] = \"numerical\" if IS_DTYPE['number'](df[c].dtype) else \"categorical\"\n",
    "            \n",
    "        return col_types\n",
    "    \n",
    "    def fill_d1_with_d2(self, d1, d2):\n",
    "        \"\"\"Add extra stats from d2 (default) to d1 (our dict)\"\"\"\n",
    "        for key in d2:\n",
    "            if (key in d1 and isinstance(d1[key], dict) and isinstance(d2[key], dict)):\n",
    "                self.deep_merge_dicts(d1[key], d2[key])\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "\n",
    "    def get_analysis(self): #generates report, saves it in self.report AND returns it.\n",
    "        \"\"\" This method will return a JSON containing the comparison between the dataset knowledge\n",
    "            and the domain knowledge\n",
    "        \"\"\"    \n",
    "\n",
    "        col_stats_comparison = {\n",
    "            \"column_stats\": self.compare_stats(self.domain_knowledge[\"column_stats\"], self.dataset_knowledge[\"column_stats\"])\n",
    "        }\n",
    "        \n",
    "        dataset_stats_comparison = {\n",
    "            \"dataset_stats\": self.compare_stats(self.domain_knowledge[\"dataset_stats\"], self.dataset_knowledge[\"dataset_stats\"])\n",
    "        }\n",
    "        comparison = dict(col_stats_comparison, **dataset_stats_comparison)\n",
    "        for key in comparison[\"column_stats\"]:\n",
    "            comparison[\"column_stats\"][key] = {\n",
    "                \"value\": comparison[\"column_stats\"][key],\n",
    "                \"score\": \"relevant\" if comparison[\"column_stats\"][key] > 0.5 else \"not_relevant\"\n",
    "            }\n",
    "\n",
    "        comparison = dict(col_stats_comparison, **dataset_stats_comparison)\n",
    "        \"\"\"TODO: FIND BETTER NAMES FFS\"\"\"\n",
    "\n",
    "        return dict(col_stats_comparison, **dataset_stats_comparison)\n",
    "    \n",
    "    def compare_stats(self,stats1, stats2):\n",
    "        \"\"\" Compares two dictionaries with the same structure \"\"\"\n",
    "        comparison = {}\n",
    "\n",
    "        if stats1:\n",
    "            for c in stats1:\n",
    "                if(stats1[c][\"metric\"] == stats1[c][\"metric\"]):\n",
    "                    #TODO: use a real comparison metric\n",
    "                    comparison[c] = stats2[c][\"value\"] - stats2[c][\"value\"]\n",
    "            return comparison\n",
    "        else:\n",
    "            return stats2\n",
    "    \n",
    "    def update_stats(self): \n",
    "        new_stats = {}\n",
    "        #TODO : customize this so weights can be other things aswell\n",
    "        n = self.domain_knowledge[\"n\"]\n",
    "        for c in self.dataset_knowledge:\n",
    "            self.domain_knowledge[c][\"value\"] = (n * self.domain_knowledge[c][\"value\"] + self.dataset_knowledge[c][\"value\"]) / (n + 1)\n",
    "        \n",
    "        #TODO : customize this so you're able to take into account the number of rows\n",
    "        self.domain_knowledge[\"n\"] = n + 1\n",
    "        \n",
    "        #Save the information\n",
    "        self.std.knowledge = self.domain_knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBR Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreCBR(StoreDomain):\n",
    "    \"\"\" StoreDomain but for human knowledge. Extends StoreDomain to add profile functionality\n",
    "        and CBR-based behaviour.\n",
    "        Each storecbr is tied to a profile and it can only modify it\n",
    "    \"\"\"\n",
    "    def __init__(self, attribute_list, profile, path, domain_name=None):\n",
    "        \"\"\" Handy initialization \"\"\"\n",
    "        self.load(attribute_list, profile, path, domain_name)\n",
    "        \n",
    "    def load(self, attribute_list, profile, path, domain_name):\n",
    "        \"\"\" Given a dataset searches for a domain in the specified folder(s) and loads\n",
    "        \"\"\"\n",
    "        for fnm in glob.glob(os.path.join(path,\"*.json\")):\n",
    "            with open(fnm) as f:\n",
    "                data = json.load(f)\n",
    "            if data[\"attribute_list\"] == attribute_list:\n",
    "                self.profile = profile\n",
    "                self.domain_name = data[\"domain_name\"] if \"domain_name\" in data else str(attribute_list)\n",
    "                self.attribute_list = data[\"attribute_list\"] if \"attribute_list\" in data else self.attribute_list\n",
    "                self.profiles = data[\"profiles\"] \n",
    "                self.fnm = fnm\n",
    "                break;\n",
    "                        \n",
    "        if not hasattr(self,\"fnm\"):\n",
    "            self.set_defaults(domain_name ,attribute_list, path, profile)\n",
    "            \n",
    "    def set_defaults(self, domain_name ,attribute_list, path, profile):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.domain_name = domain_name or attribute_list\n",
    "        self.attribute_list = attribute_list\n",
    "        self.profile = profile\n",
    "\n",
    "        self.fnm = glob.glob(os.path.join( path, self.domain_name+\".json\") )\n",
    "        self.knowledge = {\n",
    "            \"profiles\": {\n",
    "                \"default\" : {},\n",
    "                profile : {\n",
    "                    \"profile_knowledge\" : {\n",
    "                        \"column_stats\": {},\n",
    "                        \"dataset_stats\": {}\n",
    "                    },\n",
    "                    \"graphical_knowledge\": {}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        #self.knowledge[\"profiles\"][profile] = {\n",
    "        #    \"profile_knowledge\" : {}\n",
    "        #}\n",
    "        \n",
    "    def modify(self, new_info):\n",
    "        self.profiles[self.profile] = new_info\n",
    "        \n",
    "    def run_tournament(self): pass\n",
    "    \"\"\"ELO \"\"\"    \n",
    "    def interaction_learning(self): pass\n",
    "    \"\"\" despues\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Another kind of information is stored about the domains. This is the \n",
    "    information concerning the **human** side of things, that is, **how** \n",
    "    to interpret these stats and turn them into something that humans with different levels of \n",
    "    familiarity can understand.\n",
    "    To do this, we provide use another storage class that will contain human-relevant data that\n",
    "    will modify the objective comparison delivered by the analysis module.\n",
    "\"\"\"\n",
    "\n",
    "class Reporter():\n",
    "    \n",
    "    def __init__(self, df, profile, domain_path, human_knowledge_path):\n",
    "        self.profile = profile\n",
    "        self.dsa = DatasetAnalyzer(df, path=domain_path)\n",
    "        self.scbr = StoreCBR(list(df.columns), profile, path=human_knowledge_path, domain_name=df.name)\n",
    "        self.generate()\n",
    "        \n",
    "    def generate(self):\n",
    "\n",
    "        self.analysis = self.dsa.get_analysis()\n",
    "        self.dataset_data = self.dsa.dataset_knowledge\n",
    "        self.human_knowledge = self.scbr.profiles[self.scbr.profile]\n",
    "        self.report = {\"dataset\":{},\"columns\":{},\"dataset_profile\":{},\"columns_profile\":{}, \"graphical_information\":{}}\n",
    "        print(self.analysis)\n",
    "        \"\"\"Comparison between domain data and profile data : how much does this user care about...\"\"\"\n",
    "        for k in self.analysis[\"dataset_stats\"].keys():\n",
    "            self.report[\"dataset\"][k] = {\n",
    "                \"value\": self.analysis[\"dataset_stats\"][k][\"value\"] * self.human_knowledge[\"profile_knowledge\"][\"dataset_stats\"][k][\"value\"],\n",
    "                \"score\": \"TODO\"\n",
    "            }\n",
    "            \n",
    "        for k in self.analysis[\"column_stats\"].keys():\n",
    "            self.report[\"columns\"][k] = {\n",
    "                \"value\": self.analysis[\"column_stats\"][k][\"value\"] * self.human_knowledge[\"profile_knowledge\"][\"column_stats\"][k][\"value\"],\n",
    "                \"score\": \"TODO\"\n",
    "            }\n",
    "            \n",
    "        \"\"\"Comparison between profile data and profile data : analysis but user vs itself historical...\"\"\"\n",
    "        for k in self.human_knowledge[\"profile_knowledge\"][\"dataset_stats\"].keys():\n",
    "            self.report[\"dataset_profile\"][k] = {\n",
    "                \"value\": self.human_knowledge[\"profile_knowledge\"][\"dataset_stats\"][k][\"value\"] * self.dataset_data[\"dataset_stats\"][k][\"value\"],\n",
    "                \"score\": \"TODO\"\n",
    "\n",
    "            }\n",
    "        \n",
    "        for k in self.human_knowledge[\"profile_knowledge\"][\"column_stats\"].keys():\n",
    "            self.report[\"columns_profile\"][k] = {\n",
    "                \"value\": self.human_knowledge[\"profile_knowledge\"][\"column_stats\"][k][\"value\"] * self.dataset_data[\"column_stats\"][k][\"value\"],\n",
    "                \"score\": \"TODO\"\n",
    "\n",
    "            }\n",
    "        \"\"\" Graphical Information \"\"\"\n",
    "        for k in self.human_knowledge[\"graphical_knowledge\"].keys():\n",
    "            self.report[\"graphical_information\"][k] = {\n",
    "                \"color\" : self.human_knowledge[\"graphical_knowledge\"][k][\"color\"],\n",
    "                \"graph\" :  self.human_knowledge[\"graphical_knowledge\"][k][\"graph\"]\n",
    "            }\n",
    "\n",
    "    def modify(self,new_info):\n",
    "        self.human_info.modify(new_info)\n",
    "    def save_human_info(self,new_info):\n",
    "        self.human_info.save(new_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ['id', 'age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio'] ['id', 'age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']\n",
      "{'column_stats': {'id': {'value': 0.0, 'score': 'not_relevant'}, 'age': {'value': 0.0, 'score': 'not_relevant'}, 'gender': {'value': 0.0, 'score': 'not_relevant'}, 'height': {'value': 0.0, 'score': 'not_relevant'}, 'weight': {'value': 0.0, 'score': 'not_relevant'}, 'ap_hi': {'value': 0.0, 'score': 'not_relevant'}, 'ap_lo': {'value': 0.0, 'score': 'not_relevant'}, 'cholesterol': {'value': 0.0, 'score': 'not_relevant'}, 'gluc': {'value': 0.0, 'score': 'not_relevant'}, 'smoke': {'value': 0.0, 'score': 'not_relevant'}, 'alco': {'value': 0.0, 'score': 'not_relevant'}, 'active': {'value': 0.0, 'score': 'not_relevant'}, 'cardio': {'value': 0.0, 'score': 'not_relevant'}}, 'dataset_stats': {}}\n",
      "CPU times: user 102 ms, sys: 27.7 ms, total: 130 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"data/cardio_train.csv\",sep=\";\")\n",
    "df.name = \"medical_data\"\n",
    "\n",
    "#Set cols as categorical\n",
    "cat_cols = [\"cholesterol\", \"gluc\", \"smoke\",  \"alco\", \"active\", \"cardio\", \"gender\"]\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "df.head()\n",
    "repo = Reporter(df, profile=\"patient\", domain_path=\"./domain_storage/\", human_knowledge_path=\"./human_storage/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ['id', 'age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio'] ['week', 'weekday', 'M0700', 'M0930', 'M1300', 'M1500', 'M1800', 'M2000', 'M2300', 'i0700', 'i0930', 'i1300', 'i1500', 'i1800', 'i2300']\n",
      "      ['week', 'weekday', 'M0700', 'M0930', 'M1300', 'M1500', 'M1800', 'M2000', 'M2300', 'i0700', 'i0930', 'i1300', 'i1500', 'i1800', 'i2300'] ['week', 'weekday', 'M0700', 'M0930', 'M1300', 'M1500', 'M1800', 'M2000', 'M2300', 'i0700', 'i0930', 'i1300', 'i1500', 'i1800', 'i2300']\n",
      "{'column_stats': {'week': {'value': 0.0, 'score': 'not_relevant'}, 'weekday': {'value': 0.0, 'score': 'not_relevant'}, 'M0700': {'value': 0.0, 'score': 'not_relevant'}, 'M0930': {'value': 0.0, 'score': 'not_relevant'}, 'M1300': {'value': 0.0, 'score': 'not_relevant'}, 'M1500': {'value': 0.0, 'score': 'not_relevant'}, 'M1800': {'value': 0.0, 'score': 'not_relevant'}, 'M2000': {'value': 0.0, 'score': 'not_relevant'}, 'M2300': {'value': 0.0, 'score': 'not_relevant'}, 'i0700': {'value': 0.0, 'score': 'not_relevant'}, 'i0930': {'value': 0.0, 'score': 'not_relevant'}, 'i1300': {'value': 0.0, 'score': 'not_relevant'}, 'i1500': {'value': 0.0, 'score': 'not_relevant'}, 'i1800': {'value': 0.0, 'score': 'not_relevant'}, 'i2300': {'value': 0.0, 'score': 'not_relevant'}}, 'dataset_stats': {}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1c59a09c09c3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, profile, domain_path, human_knowledge_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStoreCBR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman_knowledge_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1c59a09c09c3>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_knowledge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graphical_knowledge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             self.report[\"graphical_information\"][k] = {\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0;34m\"color\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_knowledge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graphical_knowledge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;34m\"graph\"\u001b[0m \u001b[0;34m:\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_knowledge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graphical_knowledge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'color'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"data/diabetic.csv\")\n",
    "df = df.fillna(0)\n",
    "df.name = \"diabetic\"\n",
    "del df[\"Notes\"]\n",
    "del df[\"ID\"]\n",
    "cat_cols = [\"week\",\"weekday\"]\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "df.head()\n",
    "repo = Reporter(df, profile=\"patient\", domain_path=\"./domain_storage/\", human_knowledge_path=\"./human_storage/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': {'active': {'score': 'TODO', 'value': 0.0},\n",
       "  'age': {'score': 'TODO', 'value': 0.0},\n",
       "  'alco': {'score': 'TODO', 'value': 0.0},\n",
       "  'ap_hi': {'score': 'TODO', 'value': 0.0},\n",
       "  'ap_lo': {'score': 'TODO', 'value': 0.0},\n",
       "  'cardio': {'score': 'TODO', 'value': 0.0},\n",
       "  'cholesterol': {'score': 'TODO', 'value': 0.0},\n",
       "  'gender': {'score': 'TODO', 'value': 0.0},\n",
       "  'gluc': {'score': 'TODO', 'value': 0.0},\n",
       "  'height': {'score': 'TODO', 'value': 0.0},\n",
       "  'id': {'score': 'TODO', 'value': 0.0},\n",
       "  'smoke': {'score': 'TODO', 'value': 0.0},\n",
       "  'weight': {'score': 'TODO', 'value': 0.0}},\n",
       " 'columns_profile': {'active': {'score': 'TODO', 'value': 0.3},\n",
       "  'age': {'score': 'TODO', 'value': 6758.129000000001},\n",
       "  'alco': {'score': 'TODO', 'value': 0.0},\n",
       "  'ap_hi': {'score': 'TODO', 'value': 27.6},\n",
       "  'ap_lo': {'score': 'TODO', 'value': 18.400000000000002},\n",
       "  'cardio': {'score': 'TODO', 'value': 0.0},\n",
       "  'cholesterol': {'score': 'TODO', 'value': 0.3},\n",
       "  'gender': {'score': 'TODO', 'value': 0.245},\n",
       "  'gluc': {'score': 'TODO', 'value': 0.3},\n",
       "  'height': {'score': 'TODO', 'value': 23.924999999999997},\n",
       "  'id': {'score': 'TODO', 'value': 23500.704999999998},\n",
       "  'smoke': {'score': 'TODO', 'value': 0.0},\n",
       "  'weight': {'score': 'TODO', 'value': 10.44}},\n",
       " 'dataset': {},\n",
       " 'dataset_profile': {},\n",
       " 'graphical_information': {}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ['id', 'age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio'] ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "      ['week', 'weekday', 'M0700', 'M0930', 'M1300', 'M1500', 'M1800', 'M2000', 'M2300', 'i0700', 'i0930', 'i1300', 'i1500', 'i1800', 'i2300'] ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "      ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'] ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "{'column_stats': {'age': {'value': 0.0, 'score': 'not_relevant'}, 'sex': {'value': 0.0, 'score': 'not_relevant'}, 'cp': {'value': 0.0, 'score': 'not_relevant'}, 'trestbps': {'value': 0.0, 'score': 'not_relevant'}, 'chol': {'value': 0.0, 'score': 'not_relevant'}, 'fbs': {'value': 0.0, 'score': 'not_relevant'}, 'restecg': {'value': 0.0, 'score': 'not_relevant'}, 'thalach': {'value': 0.0, 'score': 'not_relevant'}, 'exang': {'value': 0.0, 'score': 'not_relevant'}, 'oldpeak': {'value': 0.0, 'score': 'not_relevant'}, 'slope': {'value': 0.0, 'score': 'not_relevant'}, 'ca': {'value': 0.0, 'score': 'not_relevant'}, 'thal': {'value': 0.0, 'score': 'not_relevant'}, 'target': {'value': 0.0, 'score': 'not_relevant'}}, 'dataset_stats': {}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'profile_knowledge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1c59a09c09c3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, profile, domain_path, human_knowledge_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStoreCBR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman_knowledge_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1c59a09c09c3>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_stats\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             self.report[\"columns\"][k] = {\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_stats\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_knowledge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"profile_knowledge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"column_stats\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"TODO\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'profile_knowledge'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "\n",
    "df.name = \"heart\"\n",
    "cat_cols = [\"sex\",\"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "df.head()\n",
    "repo = Reporter(df, profile=\"patient\", domain_path=\"./domain_storage/\", human_knowledge_path=\"./human_storage/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': {'active': {'score': 'TODO', 'value': 0.0},\n",
       "  'age': {'score': 'TODO', 'value': 0.0},\n",
       "  'alco': {'score': 'TODO', 'value': 0.0},\n",
       "  'ap_hi': {'score': 'TODO', 'value': 0.0},\n",
       "  'ap_lo': {'score': 'TODO', 'value': 0.0},\n",
       "  'cardio': {'score': 'TODO', 'value': 0.0},\n",
       "  'cholesterol': {'score': 'TODO', 'value': 0.0},\n",
       "  'gender': {'score': 'TODO', 'value': 0.0},\n",
       "  'gluc': {'score': 'TODO', 'value': 0.0},\n",
       "  'height': {'score': 'TODO', 'value': 0.0},\n",
       "  'id': {'score': 'TODO', 'value': 0.0},\n",
       "  'smoke': {'score': 'TODO', 'value': 0.0},\n",
       "  'weight': {'score': 'TODO', 'value': 0.0}},\n",
       " 'columns_profile': {'active': {'score': 'TODO', 'value': 0.3},\n",
       "  'age': {'score': 'TODO', 'value': 6758.129000000001},\n",
       "  'alco': {'score': 'TODO', 'value': 0.0},\n",
       "  'ap_hi': {'score': 'TODO', 'value': 27.6},\n",
       "  'ap_lo': {'score': 'TODO', 'value': 18.400000000000002},\n",
       "  'cardio': {'score': 'TODO', 'value': 0.0},\n",
       "  'cholesterol': {'score': 'TODO', 'value': 0.3},\n",
       "  'gender': {'score': 'TODO', 'value': 0.245},\n",
       "  'gluc': {'score': 'TODO', 'value': 0.3},\n",
       "  'height': {'score': 'TODO', 'value': 23.924999999999997},\n",
       "  'id': {'score': 'TODO', 'value': 23500.704999999998},\n",
       "  'smoke': {'score': 'TODO', 'value': 0.0},\n",
       "  'weight': {'score': 'TODO', 'value': 10.44}},\n",
       " 'dataset': {},\n",
       " 'dataset_profile': {},\n",
       " 'graphical_information': {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontend module\n",
    "Logic-free, just turns an information json to a user-readable report with images, which then prints out for the user to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-0b67c57123be>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-0b67c57123be>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def red_json(self,):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class TextTemplates:\n",
    "    def __init__(self):\n",
    "    def red_json(self,):\n",
    "PRETTY_JSON = {\n",
    "    \"graphs\": {\n",
    "        \"some_col\":{\n",
    "            \"type\" : \"bars\",\n",
    "            \"cat_colors\" : {\n",
    "                \"cat\" : \"red\",\n",
    "                \"dog\" : \"blue\",\n",
    "            },\n",
    "            \"bg_color\" : \"white\"\n",
    "        }\n",
    "    },\n",
    "    \"text\" : {\n",
    "        \"some_col_avg\" : {\n",
    "            \"reason\" : \"high\",\n",
    "            \"number_ds\" : 1.3,\n",
    "            \"number_domain\": 0.2\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_CLASSES = {\n",
    "    \"bars\" : \"asdasd\",\n",
    "    \"pie\" : \"asdasd\",\n",
    "    \"timeline\" : \"\"\n",
    "    #etc\n",
    "}\n",
    "class FrontEnd:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Self explaining, associate graph with \"\"\"\n",
    "        #self.graphs_config = config[\"graphs\"]\n",
    "       # self.text_config = config[\"text\"]\n",
    "    def generate_graphs(self):\n",
    "        for config in self.graphs_config:\n",
    "            print(config)\n",
    "    def generate_text(self):\n",
    "        for config in self.text_config:\n",
    "            print()\n",
    "        #text things\n",
    "        #graphical things\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TournamentMaker():\n",
    "    def __init__(self,seed, frontender): pass\n",
    "    def present_candidate(self,report):pass #\n",
    "    def organize_tournament(self):pass # generate tree\n",
    "    def set_votes(self,votes):pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_PARAMS = {\n",
    "    \"domain_name\" : \"animals\",\n",
    "    \"attribute_list\": [] ,\n",
    "    \"knowledge\" : {}\n",
    "}\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"First we read our data\"\"\"\n",
    "    df = pd.read_csv(\"nonsense.csv\")\n",
    "    #Do everything from the reporter class to get the report\n",
    "    #Feed the report to the frontend class for the pretty representation\n",
    "    print(\"henlo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
