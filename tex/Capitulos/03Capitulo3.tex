%---------------------------------------------------------------------
%
%          Capï¿½tulo 2
%
%---------------------------------------------------------------------

\chapter{The Program Implementation : Architecture}


\begin{resumen}
In this chapter we provide a technical analysis of our tool, examining how it works and what was designed for at a programming level, aswell as its module structure.
\end{resumen}

\linespread{1.6}

%-------------------------------------------------------------------
\section{The Program Module Structure}
%-------------------------------------------------------------------

First we need to provide our program with a clear programming structure that allows it to be customizable enough, as it is very important that we're able to add new functionality related to new kinds of analysis, users or datasets.

To make it as customizable as possible, we've divided it in modules that have a clear functionality, with the objective that we're able to change an aspect of the program by changing just one module and not the whole program.

All the code is openly hosted at www.github.com/jorses/tfg.

We will have one module which handles the loading and storing of the objective information about domains, one that does the same for user profiles associated to each domain, one that runs an analysis on the dataset and compares it to the domain, and one that puts everything together to generate a report which will be then presented to the user through the frontend module, which handles interactions with the user.

So our hierarchy goes as follows, the frontend module takes input from the user regarding its profile (input done through a dictionary-like object in the proof of concept) and a path to the dataset that is to be analyzed.

This module then relays the information to the two modules that handle information storage. 

The storage module will read the dataset, as a CSV file in the proof of concept, read its columns and search its database for a matching domain information.

In a more advanced version this search would be performed through a lookup on a non relational database like MongoDB, but for the proof of concept it looks in a folder where it stores the domain information objects as JSONs.

When it finds one that matches the column names, it loads the object into a python dictionary and sends it to the analysis module along with the new dataset.

The analysis module then performs an analysis and generates a proto-report with all the objective information about the comparison between the dataset analysis and the domain historical analysis, then passes this along to the report generation module.

In parallel, the module that handles the profile will load it in a way similar to the storage module, then pass it along to the report generation module.

When the report generation module gets all the needed information uses the user-generated profile to "filter" the objective results (as explained later in detail), and extract from them the final info that will then be presented to the user through the frontend module.

The user is able to make changes to this final report (through the console in the proof of concept), and before closing the program

All of this enables the CBR process which is explained in chapters 5 and 6.

%-------------------------------------------------------------------
\section{Non relational databases and the JSON structure}
%-------------------------------------------------------------------
\label{cap2:sec:jsonfiller}

For the proof of concept implementation developed in this thesis, we've used the JSON structure to handle our information, to write it to a file system and to read it later.

JSON, short for JavaScript Object Notation is a file format that uses text readable by humans to write attribute value pairs and serializable values. 

It is a a very common data format used for general storage of information, and will allow us to do a quick implementation capable of handling enough domains and profiles to test our program, and will allow us to later transfer this information to a non-relational database system such as MongoDB.

Python also allows us to quickly load this kind of object into its native dictionary type, making it easy for our modules to load, manipulate and store the information.

A non relational database is one that does not use the tabular schema of rows and columns found in most typical databases like SQL.

Our need for such a database comes from the fact that not every domain will have the same structure of knowledge for its analysis, with the possibilities of combinations being almost infinite and thus negating any approach to putting it inside a tabular schema.

Inside these databases data may be kept as JSON documents, which is the format that we've chosen.

The proposed database for the final model is MongoDB. 

MongoDB is a non relational database program, which is also cross-platform and document oriented. It works with objects very similar to JSON, and is licensed under the Server Side Public License (SSPL). 

Aside from this, it's easily managed through Python and has native packages to communicate with it, which would make our life easier when implementing the full program.

All of this provides us with a solution catered to our needs, which can be scalated easily if needed into a production ready state, capable of dealing with large amounts of data and providing a real solution to real users.

Every module listed below corresponds to a Python class, and has a number of methods, functions and dictionaries associated to it to perform its function.

%-------------------------------------------------------------------
\section{The Objective Storage Module}
%-------------------------------------------------------------------
\label{cap2:sec:objjson}

This module is capable of reading and updating the information stored about the objective analysis of past domains.

It sits at the lowest point in the hierarchy, as it only does as commanded and is usually handled by the other modules and used as a mere tool to get information, similar to the profile storage module.

Both modules are derived from a more basic Python class, called just Storage, from which they inherit the methods responsible from loading and storing files (or updating and loading from the database, in the scalable version).

This class adds to these functions more advanced tools to deal with the domain information and load it into a dictionary-like object that will then be able to be easily handled by the analysis module, translating information stored in plain text like function names to its equivalent variables and Python dictionaries in the program, using the dictionaries which it has as an attribute.

This process is reversed before writing the information back to disk in a format that can be condensed to JSON documents, mainly turning every function name and other serializable objects back into strings to be stored.

%-------------------------------------------------------------------
\section{The Profile Storage Module}
%-------------------------------------------------------------------
\label{cap2:sec:profilestorage}

Another kind of information is stored about the domains. This is the information concerning the human side of things, that is, how to interpret these stats and turn them into something that humans with different levels of familiarity can understand.

To do this, we provide use another storage class that will contain human-relevant data that will modify the objective comparison delivered by the analysis module.

A system of profiles is added to the object itself, inside a "profiles" key. The domain associated is clear as they share the same "attributelist" identification system.

The information contained in each one of these "profiles" serves two different purposes. It provides customizable elements of  how  the data will be presented to the user, and it keeps an historical record of this user's dataset results (similar to the one in the domain storage) making a historical following of a profile possible.

For each domain, there's a default profile. This provides a way to present the data when no previous knowledge of the profile is available. The automated processes of obtaining this profile and tuning the existing profiles from user feedback will be explained in further chapters.

%-------------------------------------------------------------------
\section{The Comparison Metrics}
%-------------------------------------------------------------------
\label{cap2:sec:metrics}

The purpose of these functions is to provide a way to measure the properties of a given dataset or knowledge domain.

We have to note first that not every kind of possible metric has been added to the program, but we've instead added enough to cover the most common types of analysis, mainly detecting distributions, most frequent values, and calculating a series of common metrics over numerical columns.

However, adding a new metric is as simple as providing it with a name, an associated function which fits into the types of one of the metrics listed below, and adding the pair of name,function to a dictionary present in the code.

The rest of the program will behave exactly the same, thus fittinng the design principle mentioned before of being able to expand the program quickly and efficiently.

We can categorize them as follows:

First the "measurement" metrics, used to get the information of a single dataset or domain.
\begin{itemize}
\item Dataset Metrics : they concern the dataset as a whole, like number of rows with missing values.
    dm :: (ds) --> num
    
\item Single Column Metrics : they concern a certain column, and are based on the type of the column.
    For numerical columns we will have things like median, averages, deviations, distributions...
    For categorical columns we'll work with frequencies and things of the sort.
    scm :: (col) --> num
    
\item Multiple Column Metrics : we will be looking for correlations and things of that sort.
    scm :: (col,col) --> num 
    Time based metrics will be defined from this construct.
\end{itemize}

We will also have "comparison" metrics, used to compare datasets against their domains.
These metrics will compare the output of two measurement metrics, both will have to
spawn from the same function.
\begin{itemize}
\item compm :: (metric) --> num
\end{itemize}

Note that these metrics are not to provide "meaning" or any human-readable input, nor to be
inherently comparable between each other outside of a framework of understanding of the domain
(metric importance).

A mean to convert these machine cold metrics into human understanding will be provided in further 
modules. For now, we're not taking humans into account.

%-------------------------------------------------------------------
\section{The Analysis Module}
%-------------------------------------------------------------------
\label{cap2:sec:analysis}

The analysis module will receive a dataset, use the Storage module to load its information, then analyze the dataset, which generates a similar object to the domain json, then producing a comparison of both.

The main methods for this module are  getstats ,  getcolumnstats  and  getdatasetstats .

The  getstats  method is just a wrapper for the other two, calls them both and stores its results inside the Analyzer class.

Both  getcolumnstats  and  getdatasetstats  compute the statistics for the given dataset. If there is previous knowledge of the domain, the stats that appear there are computed for the domain. If not, a standard set of frequencies for categorical values and medians and distributions for numerical values are calculated and used to populate the stats object.

The most important method is  getanalysis . Once the stats for the datasets have been generated, if there's previous knowledge available the class runs an analysis comparing the metrics of the two, and generating an object with the result. For this to happen, each metric defined for the dataset must have an associated  comparison metric .

If there is no previous knowledge then the dataset stats are passed along to the reporter with a field indicating that there was no previous knowledge.

In any case, at this level we've already filtered what is relevant and what is not from the comparison.

%-------------------------------------------------------------------
\section{The Report Generation Module}
%-------------------------------------------------------------------
\label{cap2:sec:reporter}

This module stands at the edge between the backend and the frontend. It receives the information from the comparison between the dataset and the domain knowledge and extracts the relevant profile information.

Once this is done, it uses both to generate a report with all the information from both sides. The user-relevant info will modify what is shown and  how  that is shown, changing the graphical elements according to the user so the frontend modules are able to be logic-free.

It is able to directly modify the profile information by the proxy methods  modify  and  savehumaninfo . Its main method,  generate , will create and populate an attribute within itself called report.

This method is called when the class itself is generated but the report can be modified as any Python attribute if needed.

At this point, we have an object that represents the dataset compared to the historical data and data about the profile associated with the user. This information, however, is in the form of a JSON object and is not really human-readable. The job of the frontend modules is to take this information and turn it into something easy to understand.

%-------------------------------------------------------------------
\section{The Frontend Module}
%-------------------------------------------------------------------
\label{cap2:sec:frontend}

The purpose of this module is to handle the user program interaction. It sits atop of the program hierarchy, being able to use all the other modules as it sees fit.

Its role begins and the very beginning of the program life cycle and ends at the same time the user decides to exit. 

Its functionality is based on the principles of minimalistic design and the user being able to interact with every element of the presented report.

For our proof of concept, a more simple design has been put in place. Instead of clicking on the elements, the user is able to interact with the program through the console.

However, all the functionality is present, as the users can input the information the program requests to generate the profile, and then change the report presented to them using commands too.

When the program is closed the frontend module passes the modified report to the information storage modules, with both the updated objective information and the updated subjective information.